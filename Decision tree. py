# Load libraries
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn import datasets
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from scipy import interp
from matplotlib import pyplot as plt
import itertools
from sklearn.metrics import plot_confusion_matrix
from sklearn import svm
from sklearn.neural_network import MLPClassifier

def get_auc_scores(clf, X_train, X_test, y_train, y_test):
   # Prints the AUC scores for training and testing data and returns testing score”””
   y_train_score = clf.predict_proba(X_train)[:, 1]
   y_test_score = clf.predict_proba(X_test)[:, 1]
   auc_train = roc_auc_score(y_train, y_train_score)
   auc_test = roc_auc_score(y_test, y_test_score)
   print("Training AUC:", auc_train)
   print("Testing AUC:", auc_test)
   return y_test_score

def plot_roc_curve(y_test, y_test_score):
    # Plot ROC curve for testing data”””
    fpr, tpr, _ = roc_curve(y_test, y_test_score)
    roc_auc = auc(fpr, tpr)
    plt.figure(dpi=1200)
    plt.plot(fpr, tpr, label="Decision tree ROC curve (area = %0.2f)" % roc_auc)
    plt.plot([0, 1], [0, 1], "k-")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Receiver operating characteristic curve")
    plt.legend(loc="lower right")
    plt.show()
 

   
# load data
#new_col_names = ['recurhcc', 'HCCrecur_1yr', 'HCCrecur_2yr', 
#       'HCCrecur_3yr', 'HCCrecur_4yr', 'HCCrecur_5yr', 'ST', 'OS_2yr', 'OS_5yr', 'afpb_copy', 'afpb_cutoff_400', 'afpb_cutoff_100',
#       'ntumour_copy', 'Tumor_number_3', 'stumor_copy', 'Tumor_size_3cm',
#       'vasper_2', 'Macro_copy', 'differen_copy', 'Differen_defined', 'EGF',
#       'FGF2', 'EOTAXIN', 'TGFa', 'GCSF', 'FLT3L', 'GMCSF', 'FRACTALKINE',
#       'IFNa2', 'IFNa2_complications', 'IFNg', 'GRO', 'IL10', 'MCP3',
#       'IL12P40', 'MDC', 'IL12P70', 'PDGFAA', 'IL13', 'PDGFAB_BB', 'IL15',
#       'SCD40L', 'IL17A', 'IL1RA', 'IL1a', 'IL9', 'IL1b', 'IL2',
#       'IL2_complications', 'IL3', 'IL4', 'IL5', 'IL6', 'IL7', 'IL8', 'IP10',
#       'MCP1', 'MIP1a', 'MIP1b', 'RANTES', 'TNFa', 'TNFb', 'VEGF', 'EVENTP',
#       'DFS_2yr', 'DFS_5yr', 'milan_copy', 'UCSF_copy', 'RETREAT_Score', 'Up_to_Seven', 'AFP_Model_Score', 'P3C_Score_Total_EO_IF_IL', 'Score_P3C_UCSF_AFP_2']

new_col_names = ['afpb_copy', 'afpb_cutoff_400', 'afpb_cutoff_100',
       'ntumour_copy', 'Tumor_number_3', 'stumor_copy', 'Tumor_size_3cm',
       'vasper_2', 'Macro_copy', 'differen_copy', 'Differen_defined', 'EGF',
       'FGF2', 'EOTAXIN', 'TGFa', 'GCSF', 'FLT3L', 'GMCSF', 'FRACTALKINE',
       'IFNa2', 'IFNa2_complications', 'IFNg', 'GRO', 'IL10', 'MCP3',
       'IL12P40', 'MDC', 'IL12P70', 'PDGFAA', 'IL13', 'PDGFAB_BB', 'IL15',
       'SCD40L', 'IL17A', 'IL1RA', 'IL1a', 'IL9', 'IL1b', 'IL2',
       'IL2_complications', 'IL3', 'IL4', 'IL5', 'IL6', 'IL7', 'IL8', 'IP10',
       'MCP1', 'MIP1a', 'MIP1b', 'RANTES', 'TNFa', 'TNFb', 'VEGF', 'EVENTP',
       'DFS_2yr', 'DFS_5yr']
outcome="HCCrecur_5yr"
new_col_names=[outcome]+new_col_names


# load dataset
path="/Users/jadonzhou/Research Projects/OneDrive/K Man HKU/Project 1/"
pima = pd.read_csv(path+"Database.csv")
pima=pima[new_col_names]
pima.head()
pima.describe()


#split dataset in features and target variable
feature_cols = new_col_names[1:len(new_col_names)]
X = pima[feature_cols]# Features
y = pima[outcome]
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test
# Create Decision Tree classifer object
#model = DecisionTreeClassifier(criterion="entropy")
model = DecisionTreeClassifier()
#model = svm.SVC()
#model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)

# Train Decision Tree Classifer
model = model.fit(X_train,y_train)
#model = model.fit(X,y)
#Predict the response for test dataset
y_pred = model.predict(X_test)

# plot plot_confusion_matrix
plot_confusion_matrix(model, X, y) 
plt.show()
#特征重要性
model.feature_importances_
[*zip(feature_cols,model.feature_importances_)]
# ROC curve
plot_roc_curve(y_test, y_pred)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("F1 score:",metrics.f1_score(y_test, y_pred))
print("Balanced accuracy:",metrics.balanced_accuracy_score(y_test, y_pred))
print("Brier score loss:",metrics.brier_score_loss(y_test, y_pred))
print("Average_precision_score:",metrics.average_precision_score(y_test, y_pred))
print("Precision_score:", metrics.precision_score(y_test, y_pred))
print("AUC score:",metrics.roc_auc_score(y_test, y_pred))
print("R2 score:",metrics.r2_score(y_test, y_pred))
print("precision_recall_fscore_support:",metrics.precision_recall_fscore_support(y_test, y_pred, beta=0.5))
print("Explained variance:",metrics.explained_variance_score(y_test, y_pred))

# visualization
from sklearn import tree
from IPython.display import Image  
import pydotplus
import graphviz 
#export_graphviz(model, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['0','1'])
dot_data = tree.export_graphviz(model,feature_names = feature_cols, out_file=None, filled=True, rounded=True)
graph = pydotplus.graphviz.graph_from_dot_data(dot_data)
Image(graph.create_png()) 
graph.set_size('"10,10!"')
graph.write_png(path+outcome+'.png')

# cross validation
from sklearn.model_selection import cross_val_score
cross_val_score(model, X, y, cv=5, scoring='recall_macro')
cross_val_score(model, X, y, cv=5, scoring='wrong_choice')

# optimal max_depth
import matplotlib.pyplot as plt
test = []
for i in range(10):
    model = DecisionTreeClassifier(max_depth=i+1,criterion="entropy",random_state=30,splitter="random")
    model = model.fit(X_train,y_train)
    score = model.score(X, y)
    test.append(score)
plt.figure(dpi=1200)
plt.plot(range(1,11),test,color="red",label="max_depth")
plt.title(outcome)
plt.xlabel("Number of trees")
plt.ylabel("Accuray")
plt.legend()
plt.show()








